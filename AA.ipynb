{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa67b8d0-f013-4ba0-a615-12247bfc9ac7",
   "metadata": {},
   "source": [
    "# Model Prototyping\n",
    "\n",
    "Building the basis for our model experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753b036f-c5b8-44cc-a653-4712b3b19d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.nn import Conv2d, AvgPool2d, ReLU, Dropout, Flatten, Linear, Sequential, Module\n",
    "from torch.optim import Adam\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "MODELS_DIR  = '/home/cxw/sonos_rirs/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e62a78-bba4-453b-9961-dba6aa7ba962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "model_dict['name'] = \"testrun2_regularization\"\n",
    "model_dict['notes'] = \"same as test run but with regularization\"\n",
    "model_dict['data_path'] = '/home/cxw/sonos_rirs/features/080122_5k_phase/feature_df.csv'\n",
    "model_dict['model_path'] = os.path.join(MODELS_DIR, model_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789d403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 尝试导入IPython\n",
    "    from IPython import get_ipython\n",
    "    # 检查是否在IPython环境下\n",
    "    if get_ipython() is not None:\n",
    "        # 加载autoreload扩展\n",
    "        %load_ext autoreload\n",
    "        # 设置autoreload为2\n",
    "        %autoreload 2\n",
    "except ImportError:\n",
    "    # 如果IPython没有被安装，则不作任何操作\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e13f9ea-8e81-447e-b228-3c1fa0b22de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %autoreload 2\n",
    "# # import volume_estimation.modeling as model_funcs\n",
    "# model_funcs.train_model(model_funcs.Baseline_Model, model_dict,\\\n",
    "#                         overwrite=True, epochs=1,log=False) #######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723b0f82-6ce6-489f-9c5a-10161015d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32000it [00:19, 1620.30it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_df = pd.read_csv(model_dict['data_path'])\n",
    "model_path = os.path.join(MODELS_DIR, model_dict['name'])\n",
    "\n",
    "dataset = []\n",
    "\n",
    "    \n",
    "def create_dataloader(feature_df, batch_size=1, log=True):\n",
    "    dataset = []\n",
    "    for row in tqdm(feature_df.iterrows()):\n",
    "        feat_file = row[1]['file_feature']\n",
    "        loaded = np.load(feat_file)\n",
    "\n",
    "        feature = loaded['feat']\n",
    "        feature = feature.reshape((feature.shape[0], feature.shape[1]))\n",
    "        feature = np.real(feature)\n",
    "\n",
    "        vol = loaded['vol']\n",
    "        if log:\n",
    "            vol = np.log10(vol)\n",
    "        dataset.append((feature, vol))\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader(feat_df, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c987b41d-7f00-41cc-bb96-c6fdedc585f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name testrun2_regularization\n",
      "notes same as test run but with regularization\n",
      "data_path /home/cxw/sonos_rirs/features/080122_5k_phase/feature_df.csv\n",
      "model_path /home/cxw/sonos_rirs/models/testrun2_regularization\n"
     ]
    }
   ],
   "source": [
    "savename = './testmodeldict.json'\n",
    "with open(savename, 'w') as f:\n",
    "    json.dump(model_dict, f)\n",
    "    \n",
    "with open(savename) as f:\n",
    "    load_dict = json.load(f)\n",
    "    \n",
    "for key in load_dict.keys():\n",
    "    print(key, load_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a66ab9-e949-4408-943c-ade51b8c2562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19200it [00:11, 1612.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6420it [00:03, 1664.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6380it [00:03, 1663.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = feat_df[feat_df['split']=='train']\n",
    "val_df = feat_df[feat_df['split']=='val']\n",
    "test_df = feat_df[feat_df['split']=='test']\n",
    "\n",
    "print(\"Creating training dataloader\")\n",
    "train_dataloader = create_dataloader(train_df, batch_size=16)        ##################################batch_size\n",
    "\n",
    "print(\"Creating validation dataloader\")\n",
    "val_dataloader = create_dataloader(val_df)\n",
    "\n",
    "print(\"Creating test dataloader\")\n",
    "test_dataloader = create_dataloader(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f680db8-27a2-4db8-84d3-2ef287da00ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([16, 30, 1997])\n",
      "Labels batch shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "# features = features.squeeze(1)\n",
    "# features = features.transpose(1,2)\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c776c9f2-fba2-4f0f-9ea2-93f35ecaeb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Baseline_Model(Module):\n",
    "#     def __init__(self, input_shape):\n",
    "#         #accepts a tuple with the height/width of the feature\n",
    "#         #matrix to set the FC layer dimensions\n",
    "#         super(Baseline_Model, self).__init__()\n",
    "        \n",
    "#         #block1\n",
    "#         Conv1 = Conv2d(1, 30, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool1 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block2\n",
    "#         Conv2 = Conv2d(30, 20, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool2 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block3\n",
    "#         Conv3 = Conv2d(20, 10, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool3 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block4\n",
    "#         Conv4 = Conv2d(10, 10, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool4 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block5\n",
    "#         Conv5 = Conv2d(10, 5, kernel_size=(3,9), stride=(1,1))\n",
    "#         Avgpool5 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block6\n",
    "#         Conv6 = Conv2d(5, 5, kernel_size=(3,9), stride=(1,1))\n",
    "#         Avgpool6 = AvgPool2d((2,2), stride=(2,2))\n",
    "\n",
    "#         #dropout\n",
    "#         dropout_layer = Dropout(p=0.5)\n",
    "#         height5 = input_shape[0] - 2\n",
    "#         height6 = (height5 - 2) // 2\n",
    "\n",
    "#         time1 = (input_shape[1] - 9) // 2\n",
    "#         time2 = (time1 - 9) // 2\n",
    "#         time3 = (time2 - 9) // 2\n",
    "#         time4 = (time3 - 9) // 2\n",
    "#         time5 = (time4 - 7) // 2\n",
    "#         time6 = (time5 - 7) // 2\n",
    "\n",
    "#         flat_dims = 5 * height6 * time6\n",
    "#         fc_layer = Linear(flat_dims, 1)\n",
    "        \n",
    "#         self.net = Sequential(\n",
    "#                     Conv1, ReLU(), Avgpool1,\n",
    "#                     Conv2, ReLU(), Avgpool2,\n",
    "#                     Conv3, ReLU(), Avgpool3,\n",
    "#                     Conv4, ReLU(), Avgpool4,\n",
    "#                     Conv5, ReLU(), Avgpool5,\n",
    "#                     Conv6, ReLU(), Avgpool6,\n",
    "#                     dropout_layer, Flatten(),\n",
    "#                     fc_layer, Flatten()\n",
    "#                 )\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.net:\n",
    "#             x = layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb270003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1d4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "random init mean\n",
      "random init var\n",
      "UniRepLKNetBlock(\n",
      "  (dwconv): DilatedReparamBlock(\n",
      "    (lk_origin): Conv2d(4, 4, kernel_size=(13, 13), stride=(1, 1), padding=(6, 6), groups=4)\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (se): SEBlock(\n",
      "    (down): Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (up): Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (nonlinear): ReLU(inplace=True)\n",
      "  )\n",
      "  (pwconv1): Sequential(\n",
      "    (0): NCHWtoNHWC()\n",
      "    (1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  )\n",
      "  (act): Sequential(\n",
      "    (0): GELU(approximate=none)\n",
      "    (1): GRNwithNHWC()\n",
      "  )\n",
      "  (pwconv2): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=4, bias=True)\n",
      "    (1): NHWCtoNCHW()\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      ")\n",
      "tensor([[[[ 1.3323e-15, -4.4409e-15, -7.1054e-15,  ..., -1.7764e-15,\n",
      "            2.2204e-16,  1.7764e-15],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.7764e-15,  ...,  0.0000e+00,\n",
      "            8.8818e-16,  4.4409e-16],\n",
      "          [ 0.0000e+00,  0.0000e+00,  8.8818e-16,  ...,  0.0000e+00,\n",
      "           -2.6645e-15, -3.5527e-15],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  8.8818e-16,  7.1054e-15,  ...,  0.0000e+00,\n",
      "            8.8818e-16, -3.5527e-15],\n",
      "          [-1.7764e-15,  0.0000e+00, -7.1054e-15,  ...,  7.1054e-15,\n",
      "            0.0000e+00,  1.7764e-15],\n",
      "          [-2.6645e-15,  8.8818e-16, -2.2204e-16,  ...,  1.7764e-15,\n",
      "            4.4409e-16, -4.4409e-16]],\n",
      "\n",
      "         [[ 0.0000e+00, -2.6645e-15, -1.7764e-15,  ..., -1.7764e-15,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-2.2204e-16,  2.6645e-15, -2.8866e-15,  ..., -8.8818e-16,\n",
      "            0.0000e+00, -8.8818e-16],\n",
      "          [-2.2204e-16,  1.7764e-15,  1.1102e-15,  ..., -8.8818e-16,\n",
      "           -1.7764e-15, -8.8818e-16],\n",
      "          ...,\n",
      "          [ 2.6645e-15,  0.0000e+00,  1.7764e-15,  ...,  2.2204e-16,\n",
      "            1.5543e-15, -2.6645e-15],\n",
      "          [ 1.1102e-15, -2.2204e-16, -1.7764e-15,  ...,  1.7764e-15,\n",
      "            3.5527e-15, -8.8818e-16],\n",
      "          [ 0.0000e+00,  4.4409e-16,  0.0000e+00,  ...,  2.2204e-16,\n",
      "           -2.2204e-15, -2.2204e-16]],\n",
      "\n",
      "         [[ 4.4409e-16, -1.3323e-15, -1.3323e-15,  ..., -8.8818e-16,\n",
      "            1.1102e-16,  0.0000e+00],\n",
      "          [-8.8818e-16,  1.7764e-15, -4.4409e-16,  ..., -4.4409e-16,\n",
      "            2.2204e-16, -2.2204e-16],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.2204e-16,\n",
      "           -6.6613e-16, -1.7764e-15],\n",
      "          ...,\n",
      "          [-4.4409e-16,  0.0000e+00,  8.8818e-16,  ..., -2.2204e-16,\n",
      "            0.0000e+00, -8.8818e-16],\n",
      "          [-6.6613e-16, -8.8818e-16, -1.7764e-15,  ...,  4.4409e-16,\n",
      "            0.0000e+00,  4.4409e-16],\n",
      "          [-8.8818e-16,  0.0000e+00,  0.0000e+00,  ..., -4.4409e-16,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.7764e-15, -1.0658e-14, -1.7764e-14,  ..., -3.5527e-15,\n",
      "           -1.7764e-15, -7.1054e-15],\n",
      "          [ 0.0000e+00,  7.1054e-15, -7.1054e-15,  ..., -7.1054e-15,\n",
      "            0.0000e+00, -7.1054e-15],\n",
      "          [ 3.5527e-15,  0.0000e+00,  0.0000e+00,  ...,  3.5527e-15,\n",
      "           -5.3291e-15, -2.1316e-14],\n",
      "          ...,\n",
      "          [-2.1316e-14, -3.5527e-15,  1.4211e-14,  ..., -1.7764e-15,\n",
      "            5.3291e-15, -7.1054e-15],\n",
      "          [-5.3291e-15,  0.0000e+00, -1.4211e-14,  ...,  1.4211e-14,\n",
      "            0.0000e+00,  3.5527e-15],\n",
      "          [-7.1054e-15,  7.1054e-15, -3.5527e-15,  ..., -1.0658e-14,\n",
      "           -1.7764e-15, -7.1054e-15]]],\n",
      "\n",
      "\n",
      "        [[[-3.5527e-15,  2.2204e-16,  3.5527e-15,  ...,  1.7764e-15,\n",
      "            2.6645e-15, -5.3291e-15],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.7764e-15,  ...,  1.7764e-15,\n",
      "            0.0000e+00, -4.4409e-16],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.4211e-14,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  8.8818e-16],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  1.7764e-15,  1.7764e-15,  ...,  0.0000e+00,\n",
      "            1.7764e-15,  0.0000e+00],\n",
      "          [ 1.7764e-15,  4.4409e-16, -1.3323e-15,  ..., -3.5527e-15,\n",
      "            0.0000e+00, -1.7764e-15],\n",
      "          [ 3.5527e-15,  4.4409e-16,  2.6645e-15,  ...,  1.7764e-15,\n",
      "           -1.7764e-15,  2.6645e-15]],\n",
      "\n",
      "         [[-1.7764e-15,  3.5527e-15,  4.4409e-15,  ...,  3.5527e-15,\n",
      "            0.0000e+00,  1.7764e-15],\n",
      "          [ 2.2204e-15,  3.5527e-15,  0.0000e+00,  ..., -6.6613e-16,\n",
      "            1.7764e-15,  1.1102e-15],\n",
      "          [ 2.6645e-15, -4.4409e-16,  1.7764e-15,  ..., -4.4409e-16,\n",
      "            1.7764e-15, -1.4433e-15],\n",
      "          ...,\n",
      "          [ 3.5527e-15, -2.6645e-15, -8.8818e-16,  ..., -8.8818e-16,\n",
      "           -4.4409e-16,  2.4980e-16],\n",
      "          [ 1.5543e-15,  0.0000e+00, -1.7764e-15,  ...,  1.7764e-15,\n",
      "           -3.1086e-15,  8.8818e-16],\n",
      "          [ 3.5527e-15, -8.8818e-16,  6.6613e-15,  ...,  1.1102e-15,\n",
      "           -1.7764e-15,  1.6653e-15]],\n",
      "\n",
      "         [[-8.8818e-16,  6.6613e-16,  0.0000e+00,  ..., -8.8818e-16,\n",
      "            4.4409e-16, -8.8818e-16],\n",
      "          [ 2.2204e-16,  8.8818e-16, -6.6613e-16,  ...,  4.4409e-16,\n",
      "            0.0000e+00,  6.6613e-16],\n",
      "          [ 0.0000e+00, -8.8818e-16, -3.5527e-15,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  2.2204e-16],\n",
      "          ...,\n",
      "          [-4.4409e-16,  8.8818e-16, -8.8818e-16,  ..., -2.2204e-16,\n",
      "            1.3323e-15,  2.2204e-16],\n",
      "          [ 0.0000e+00,  3.3307e-16,  4.4409e-16,  ..., -1.7764e-15,\n",
      "            2.2204e-16, -1.7764e-15],\n",
      "          [ 8.8818e-16, -4.4409e-16,  6.6613e-16,  ...,  2.2204e-16,\n",
      "           -1.3323e-15,  0.0000e+00]],\n",
      "\n",
      "         [[-7.1054e-15,  1.4211e-14, -1.4211e-14,  ..., -1.0658e-14,\n",
      "            3.5527e-15, -3.5527e-15],\n",
      "          [-3.5527e-15,  0.0000e+00, -1.7764e-14,  ...,  0.0000e+00,\n",
      "           -3.5527e-15,  7.1054e-15],\n",
      "          [-1.4211e-14, -1.4211e-14, -2.8422e-14,  ...,  1.4211e-14,\n",
      "           -3.5527e-15,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 3.5527e-15, -3.5527e-15, -1.7764e-14,  ..., -1.0658e-14,\n",
      "           -7.1054e-15, -3.5527e-15],\n",
      "          [ 3.5527e-15,  8.8818e-16,  1.4211e-14,  ..., -1.0658e-14,\n",
      "           -1.0658e-14, -7.1054e-15],\n",
      "          [ 0.0000e+00, -7.1054e-15,  7.1054e-15,  ...,  3.5527e-15,\n",
      "           -7.1054e-15, -3.5527e-15]]]], grad_fn=<SubBackward0>)\n",
      "tensor(3.3954e-16, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxw/anaconda3/envs/s3d_env/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition\n",
    "# Github source: https://github.com/AILab-CVC/UniRepLKNet\n",
    "# Licensed under The Apache License 2.0 License [see LICENSE for details]\n",
    "# Based on RepLKNet, ConvNeXt, timm, DINO and DeiT code bases\n",
    "# https://github.com/DingXiaoH/RepLKNet-pytorch\n",
    "# https://github.com/facebookresearch/ConvNeXt\n",
    "# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
    "# https://github.com/facebookresearch/deit/\n",
    "# https://github.com/facebookresearch/dino\n",
    "# --------------------------------------------------------'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "from timm.models.registry import register_model\n",
    "from functools import partial\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "except:\n",
    "    hf_hub_download = None      # install huggingface_hub if you would like to download models conveniently from huggingface\n",
    "\n",
    "has_mmdet = False\n",
    "has_mmseg = False\n",
    "#   =============== for the ease of directly using this file in MMSegmentation and MMDetection.\n",
    "#   =============== ignore the following two segments of code if you do not plan to do so\n",
    "#   =============== delete one of the following two segments if you get a confliction\n",
    "try:\n",
    "    from mmseg.models.builder import BACKBONES as seg_BACKBONES\n",
    "    from mmseg.utils import get_root_logger\n",
    "    from mmcv.runner import _load_checkpoint\n",
    "    has_mmseg = True\n",
    "except ImportError:\n",
    "    get_root_logger = None\n",
    "    _load_checkpoint = None\n",
    "\n",
    "# try:\n",
    "#     from mmdet.models.builder import BACKBONES as det_BACKBONES\n",
    "#     from mmdet.utils import get_root_logger\n",
    "#     from mmcv.runner import _load_checkpoint\n",
    "#     has_mmdet = True\n",
    "# except ImportError:\n",
    "#     get_root_logger = None\n",
    "#     _load_checkpoint = None\n",
    "#   ===========================================================================================\n",
    "\n",
    "\n",
    "class GRNwithNHWC(nn.Module):\n",
    "    \"\"\" GRN (Global Response Normalization) layer\n",
    "    Originally proposed in ConvNeXt V2 (https://arxiv.org/abs/2301.00808)\n",
    "    This implementation is more efficient than the original (https://github.com/facebookresearch/ConvNeXt-V2)\n",
    "    We assume the inputs to this layer are (N, H, W, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.use_bias = use_bias\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "        if self.use_bias:\n",
    "            self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        if self.use_bias:\n",
    "            return (self.gamma * Nx + 1) * x + self.beta\n",
    "        else:\n",
    "            return (self.gamma * Nx + 1) * x\n",
    "\n",
    "\n",
    "class NCHWtoNHWC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "class NHWCtoNCHW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "#================== This function decides which conv implementation (the native or iGEMM) to use\n",
    "#   Note that iGEMM large-kernel conv impl will be used if\n",
    "#       -   you attempt to do so (attempt_to_use_large_impl=True), and\n",
    "#       -   it has been installed (follow https://github.com/AILab-CVC/UniRepLKNet), and\n",
    "#       -   the conv layer is depth-wise, stride = 1, non-dilated, kernel_size > 5, and padding == kernel_size // 2\n",
    "def get_conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias,\n",
    "               attempt_use_lk_impl=True):\n",
    "    kernel_size = to_2tuple(kernel_size)\n",
    "    if padding is None:\n",
    "        padding = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "    else:\n",
    "        padding = to_2tuple(padding)\n",
    "    need_large_impl = kernel_size[0] == kernel_size[1] and kernel_size[0] > 5 and padding == (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    if attempt_use_lk_impl and need_large_impl:\n",
    "        print('---------------- trying to import iGEMM implementation for large-kernel conv')\n",
    "        try:\n",
    "            from depthwise_conv2d_implicit_gemm import DepthWiseConv2dImplicitGEMM\n",
    "            print('---------------- found iGEMM implementation ')\n",
    "        except:\n",
    "            DepthWiseConv2dImplicitGEMM = None\n",
    "            print('---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.')\n",
    "        if DepthWiseConv2dImplicitGEMM is not None and need_large_impl and in_channels == out_channels \\\n",
    "                and out_channels == groups and stride == 1 and dilation == 1:\n",
    "            print(f'===== iGEMM Efficient Conv Impl, channels {in_channels}, kernel size {kernel_size} =====')\n",
    "            return DepthWiseConv2dImplicitGEMM(in_channels, kernel_size, bias=bias)\n",
    "    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(dim, use_sync_bn=False):\n",
    "    if use_sync_bn:\n",
    "        return nn.SyncBatchNorm(dim)\n",
    "    else:\n",
    "        return nn.BatchNorm2d(dim)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block proposed in SENet (https://arxiv.org/abs/1709.01507)\n",
    "    We assume the inputs to this layer are (N, C, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, internal_neurons):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.down = nn.Conv2d(in_channels=input_channels, out_channels=internal_neurons,\n",
    "                              kernel_size=1, stride=1, bias=True)\n",
    "        self.up = nn.Conv2d(in_channels=internal_neurons, out_channels=input_channels,\n",
    "                            kernel_size=1, stride=1, bias=True)\n",
    "        self.input_channels = input_channels\n",
    "        self.nonlinear = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.adaptive_avg_pool2d(inputs, output_size=(1, 1))\n",
    "        x = self.down(x)\n",
    "        x = self.nonlinear(x)\n",
    "        x = self.up(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return inputs * x.view(-1, self.input_channels, 1, 1)\n",
    "\n",
    "def fuse_bn(conv, bn):\n",
    "    conv_bias = 0 if conv.bias is None else conv.bias\n",
    "    std = (bn.running_var + bn.eps).sqrt()\n",
    "    return conv.weight * (bn.weight / std).reshape(-1, 1, 1, 1), bn.bias + (conv_bias - bn.running_mean) * bn.weight / std\n",
    "\n",
    "def convert_dilated_to_nondilated(kernel, dilate_rate):\n",
    "    identity_kernel = torch.ones((1, 1, 1, 1)).to(kernel.device)\n",
    "    if kernel.size(1) == 1:\n",
    "        #   This is a DW kernel\n",
    "        dilated = F.conv_transpose2d(kernel, identity_kernel, stride=dilate_rate)\n",
    "        return dilated\n",
    "    else:\n",
    "        #   This is a dense or group-wise (but not DW) kernel\n",
    "        slices = []\n",
    "        for i in range(kernel.size(1)):\n",
    "            dilated = F.conv_transpose2d(kernel[:,i:i+1,:,:], identity_kernel, stride=dilate_rate)\n",
    "            slices.append(dilated)\n",
    "        return torch.cat(slices, dim=1)\n",
    "\n",
    "def merge_dilated_into_large_kernel(large_kernel, dilated_kernel, dilated_r):\n",
    "    large_k = large_kernel.size(2)\n",
    "    dilated_k = dilated_kernel.size(2)\n",
    "    equivalent_kernel_size = dilated_r * (dilated_k - 1) + 1\n",
    "    equivalent_kernel = convert_dilated_to_nondilated(dilated_kernel, dilated_r)\n",
    "    rows_to_pad = large_k // 2 - equivalent_kernel_size // 2\n",
    "    merged_kernel = large_kernel + F.pad(equivalent_kernel, [rows_to_pad] * 4)\n",
    "    return merged_kernel\n",
    "\n",
    "\n",
    "class DilatedReparamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Dilated Reparam Block proposed in UniRepLKNet (https://github.com/AILab-CVC/UniRepLKNet)\n",
    "    We assume the inputs to this block are (N, C, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, deploy, use_sync_bn=False, attempt_use_lk_impl=True):\n",
    "        super().__init__()\n",
    "        self.lk_origin = get_conv2d(channels, channels, kernel_size, stride=1,\n",
    "                                    padding=kernel_size//2, dilation=1, groups=channels, bias=deploy,\n",
    "                                    attempt_use_lk_impl=attempt_use_lk_impl)\n",
    "        self.attempt_use_lk_impl = attempt_use_lk_impl\n",
    "\n",
    "        #   Default settings. We did not tune them carefully. Different settings may work better.\n",
    "        if kernel_size == 17:\n",
    "            self.kernel_sizes = [5, 9, 3, 3, 3]\n",
    "            self.dilates = [1, 2, 4, 5, 7]\n",
    "        elif kernel_size == 15:\n",
    "            self.kernel_sizes = [5, 7, 3, 3, 3]\n",
    "            self.dilates = [1, 2, 3, 5, 7]\n",
    "        elif kernel_size == 13:\n",
    "            self.kernel_sizes = [5, 7, 3, 3, 3]\n",
    "            self.dilates = [1, 2, 3, 4, 5]\n",
    "        elif kernel_size == 11:\n",
    "            self.kernel_sizes = [5, 5, 3, 3, 3]\n",
    "            self.dilates = [1, 2, 3, 4, 5]\n",
    "        elif kernel_size == 9:\n",
    "            self.kernel_sizes = [5, 5, 3, 3]\n",
    "            self.dilates = [1, 2, 3, 4]\n",
    "        elif kernel_size == 7:\n",
    "            self.kernel_sizes = [5, 3, 3]\n",
    "            self.dilates = [1, 2, 3]\n",
    "        elif kernel_size == 5:\n",
    "            self.kernel_sizes = [3, 3]\n",
    "            self.dilates = [1, 2]\n",
    "        else:\n",
    "            raise ValueError('Dilated Reparam Block requires kernel_size >= 5')\n",
    "\n",
    "        if not deploy:\n",
    "            self.origin_bn = get_bn(channels, use_sync_bn)\n",
    "            for k, r in zip(self.kernel_sizes, self.dilates):\n",
    "                self.__setattr__('dil_conv_k{}_{}'.format(k, r),\n",
    "                                 nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=k, stride=1,\n",
    "                                           padding=(r * (k - 1) + 1) // 2, dilation=r, groups=channels,\n",
    "                                           bias=False))\n",
    "                self.__setattr__('dil_bn_k{}_{}'.format(k, r), get_bn(channels, use_sync_bn=use_sync_bn))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not hasattr(self, 'origin_bn'):      # deploy mode\n",
    "            return self.lk_origin(x)\n",
    "        out = self.origin_bn(self.lk_origin(x))\n",
    "        for k, r in zip(self.kernel_sizes, self.dilates):\n",
    "            conv = self.__getattr__('dil_conv_k{}_{}'.format(k, r))\n",
    "            bn = self.__getattr__('dil_bn_k{}_{}'.format(k, r))\n",
    "            out = out + bn(conv(x))\n",
    "        return out\n",
    "\n",
    "    def merge_dilated_branches(self):\n",
    "        if hasattr(self, 'origin_bn'):\n",
    "            origin_k, origin_b = fuse_bn(self.lk_origin, self.origin_bn)\n",
    "            for k, r in zip(self.kernel_sizes, self.dilates):\n",
    "                conv = self.__getattr__('dil_conv_k{}_{}'.format(k, r))\n",
    "                bn = self.__getattr__('dil_bn_k{}_{}'.format(k, r))\n",
    "                branch_k, branch_b = fuse_bn(conv, bn)\n",
    "                origin_k = merge_dilated_into_large_kernel(origin_k, branch_k, r)\n",
    "                origin_b += branch_b\n",
    "            merged_conv = get_conv2d(origin_k.size(0), origin_k.size(0), origin_k.size(2), stride=1,\n",
    "                                    padding=origin_k.size(2)//2, dilation=1, groups=origin_k.size(0), bias=True,\n",
    "                                    attempt_use_lk_impl=self.attempt_use_lk_impl)\n",
    "            merged_conv.weight.data = origin_k\n",
    "            merged_conv.bias.data = origin_b\n",
    "            self.lk_origin = merged_conv\n",
    "            self.__delattr__('origin_bn')\n",
    "            for k, r in zip(self.kernel_sizes, self.dilates):\n",
    "                self.__delattr__('dil_conv_k{}_{}'.format(k, r))\n",
    "                self.__delattr__('dil_bn_k{}_{}'.format(k, r))\n",
    "\n",
    "\n",
    "class UniRepLKNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 kernel_size,\n",
    "                 drop_path=0.,\n",
    "                 layer_scale_init_value=1e-6,\n",
    "                 deploy=False,\n",
    "                 attempt_use_lk_impl=True,\n",
    "                 with_cp=False,\n",
    "                 use_sync_bn=False,\n",
    "                 ffn_factor=4):\n",
    "        super().__init__()\n",
    "        self.with_cp = with_cp\n",
    "        if deploy:\n",
    "            print('------------------------------- Note: deploy mode')\n",
    "        if self.with_cp:\n",
    "            print('****** note with_cp = True, reduce memory consumption but may slow down training ******')\n",
    "\n",
    "        if kernel_size == 0:\n",
    "            self.dwconv = nn.Identity()\n",
    "        elif kernel_size >= 7:\n",
    "            self.dwconv = DilatedReparamBlock(dim, kernel_size, deploy=deploy,\n",
    "                                              use_sync_bn=use_sync_bn,\n",
    "                                              attempt_use_lk_impl=attempt_use_lk_impl)\n",
    "\n",
    "        else:\n",
    "            assert kernel_size in [3, 5]\n",
    "            self.dwconv = get_conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=kernel_size // 2,\n",
    "                                     dilation=1, groups=dim, bias=deploy,\n",
    "                                     attempt_use_lk_impl=attempt_use_lk_impl)\n",
    "\n",
    "        if deploy or kernel_size == 0:\n",
    "            self.norm = nn.Identity()\n",
    "        else:\n",
    "            self.norm = get_bn(dim, use_sync_bn=use_sync_bn)\n",
    "\n",
    "        self.se = SEBlock(dim, dim // 4)\n",
    "\n",
    "        ffn_dim = int(ffn_factor * dim)\n",
    "        self.pwconv1 = nn.Sequential(\n",
    "            NCHWtoNHWC(),\n",
    "            nn.Linear(dim, ffn_dim))\n",
    "        self.act = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            GRNwithNHWC(ffn_dim, use_bias=not deploy))\n",
    "        if deploy:\n",
    "            self.pwconv2 = nn.Sequential(\n",
    "                nn.Linear(ffn_dim, dim),\n",
    "                NHWCtoNCHW())\n",
    "        else:\n",
    "            self.pwconv2 = nn.Sequential(\n",
    "                nn.Linear(ffn_dim, dim, bias=False),\n",
    "                NHWCtoNCHW(),\n",
    "                get_bn(dim, use_sync_bn=use_sync_bn))\n",
    "\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim),\n",
    "                                  requires_grad=True) if (not deploy) and layer_scale_init_value is not None \\\n",
    "                                                         and layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def compute_residual(self, x):\n",
    "        y = self.se(self.norm(self.dwconv(x)))\n",
    "        y = self.pwconv2(self.act(self.pwconv1(y)))\n",
    "        if self.gamma is not None:\n",
    "            y = self.gamma.view(1, -1, 1, 1) * y\n",
    "        return self.drop_path(y)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        def _f(x):\n",
    "            return x + self.compute_residual(x)\n",
    "\n",
    "        if self.with_cp and inputs.requires_grad:\n",
    "            out = checkpoint.checkpoint(_f, inputs)\n",
    "        else:\n",
    "            out = _f(inputs)\n",
    "        return out\n",
    "\n",
    "    def reparameterize(self):\n",
    "        if hasattr(self.dwconv, 'merge_dilated_branches'):\n",
    "            self.dwconv.merge_dilated_branches()\n",
    "        if hasattr(self.norm, 'running_var'):\n",
    "            std = (self.norm.running_var + self.norm.eps).sqrt()\n",
    "            if hasattr(self.dwconv, 'lk_origin'):\n",
    "                self.dwconv.lk_origin.weight.data *= (self.norm.weight / std).view(-1, 1, 1, 1)\n",
    "                self.dwconv.lk_origin.bias.data = self.norm.bias + (\n",
    "                            self.dwconv.lk_origin.bias - self.norm.running_mean) * self.norm.weight / std\n",
    "            else:\n",
    "                conv = nn.Conv2d(self.dwconv.in_channels, self.dwconv.out_channels, self.dwconv.kernel_size,\n",
    "                                 padding=self.dwconv.padding, groups=self.dwconv.groups, bias=True)\n",
    "                conv.weight.data = self.dwconv.weight * (self.norm.weight / std).view(-1, 1, 1, 1)\n",
    "                conv.bias.data = self.norm.bias - self.norm.running_mean * self.norm.weight / std\n",
    "                self.dwconv = conv\n",
    "            self.norm = nn.Identity()\n",
    "        if self.gamma is not None:\n",
    "            final_scale = self.gamma.data\n",
    "            self.gamma = None\n",
    "        else:\n",
    "            final_scale = 1\n",
    "        if self.act[1].use_bias and len(self.pwconv2) == 3:\n",
    "            grn_bias = self.act[1].beta.data\n",
    "            self.act[1].__delattr__('beta')\n",
    "            self.act[1].use_bias = False\n",
    "            linear = self.pwconv2[0]\n",
    "            grn_bias_projected_bias = (linear.weight.data @ grn_bias.view(-1, 1)).squeeze()\n",
    "            bn = self.pwconv2[2]\n",
    "            std = (bn.running_var + bn.eps).sqrt()\n",
    "            new_linear = nn.Linear(linear.in_features, linear.out_features, bias=True)\n",
    "            new_linear.weight.data = linear.weight * (bn.weight / std * final_scale).view(-1, 1)\n",
    "            linear_bias = 0 if linear.bias is None else linear.bias.data\n",
    "            linear_bias += grn_bias_projected_bias\n",
    "            new_linear.bias.data = (bn.bias + (linear_bias - bn.running_mean) * bn.weight / std) * final_scale\n",
    "            self.pwconv2 = nn.Sequential(new_linear, self.pwconv2[1])\n",
    "\n",
    "\n",
    "\n",
    "default_UniRepLKNet_A_F_P_kernel_sizes = ((3, 3),\n",
    "                                      (13, 13),\n",
    "                                      (13, 13, 13, 13, 13, 13),\n",
    "                                      (13, 13))\n",
    "default_UniRepLKNet_N_kernel_sizes = ((3, 3),\n",
    "                                      (13, 13),\n",
    "                                      (13, 13, 13, 13, 13, 13, 13, 13),\n",
    "                                      (13, 13))\n",
    "default_UniRepLKNet_T_kernel_sizes = ((3, 3, 3),\n",
    "                                      (13, 13, 13),\n",
    "                                      (13, 3, 13, 3, 13, 3, 13, 3, 13, 3, 13, 3, 13, 3, 13, 3, 13, 3),\n",
    "                                      (13, 13, 13))\n",
    "default_UniRepLKNet_S_B_L_XL_kernel_sizes = ((3, 3, 3),\n",
    "                                             (13, 13, 13),\n",
    "                                             (13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3),\n",
    "                                             (13, 13, 13))\n",
    "UniRepLKNet_A_F_P_depths = (2, 2, 6, 2)\n",
    "UniRepLKNet_N_depths = (2, 2, 8, 2)\n",
    "UniRepLKNet_T_depths = (3, 3, 18, 3)\n",
    "UniRepLKNet_S_B_L_XL_depths = (3, 3, 27, 3)\n",
    "\n",
    "default_depths_to_kernel_sizes = {\n",
    "    UniRepLKNet_A_F_P_depths: default_UniRepLKNet_A_F_P_kernel_sizes,\n",
    "    UniRepLKNet_N_depths: default_UniRepLKNet_N_kernel_sizes,\n",
    "    UniRepLKNet_T_depths: default_UniRepLKNet_T_kernel_sizes,\n",
    "    UniRepLKNet_S_B_L_XL_depths: default_UniRepLKNet_S_B_L_XL_kernel_sizes\n",
    "}\n",
    "\n",
    "class UniRepLKNet(nn.Module):\n",
    "    r\"\"\" UniRepLKNet\n",
    "        A PyTorch impl of UniRepLKNet\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: (3, 3, 27, 3)\n",
    "        dims (int): Feature dimension at each stage. Default: (96, 192, 384, 768)\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "        kernel_sizes (tuple(tuple(int))): Kernel size for each block. None means using the default settings. Default: None.\n",
    "        deploy (bool): deploy = True means using the inference structure. Default: False\n",
    "        with_cp (bool): with_cp = True means using torch.utils.checkpoint to save GPU memory. Default: False\n",
    "        init_cfg (dict): weights to load. The easiest way to use UniRepLKNet with for OpenMMLab family. Default: None\n",
    "        attempt_use_lk_impl (bool): try to load the efficient iGEMM large-kernel impl. Setting it to False disabling the iGEMM impl. Default: True\n",
    "        use_sync_bn (bool): use_sync_bn = True means using sync BN. Use it if your batch size is small. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=768,\n",
    "                 depths=(3, 3, 27, 3),\n",
    "                 dims=(96, 192, 384, 768),\n",
    "                 drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6,\n",
    "                 head_init_scale=1.,\n",
    "                 kernel_sizes=None,\n",
    "                 deploy=False,\n",
    "                 with_cp=False,\n",
    "                 init_cfg=None,\n",
    "                 attempt_use_lk_impl=True,\n",
    "                 use_sync_bn=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        depths = tuple(depths)\n",
    "        if kernel_sizes is None:\n",
    "            if depths in default_depths_to_kernel_sizes:\n",
    "                print('=========== use default kernel size ')\n",
    "                kernel_sizes = default_depths_to_kernel_sizes[depths]\n",
    "            else:\n",
    "                raise ValueError('no default kernel size settings for the given depths, '\n",
    "                                 'please specify kernel sizes for each block, e.g., '\n",
    "                                 '((3, 3), (13, 13), (13, 13, 13, 13, 13, 13), (13, 13))')\n",
    "        print(kernel_sizes)\n",
    "        for i in range(4):\n",
    "            assert len(kernel_sizes[i]) == depths[i], 'kernel sizes do not match the depths'\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        print('=========== drop path rates: ', dp_rates)\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        self.downsample_layers.append(nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0] // 2, kernel_size=3, stride=2, padding=1),\n",
    "            LayerNorm(dims[0] // 2, eps=1e-6, data_format=\"channels_first\"),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dims[0] // 2, dims[0], kernel_size=3, stride=2, padding=1),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")))\n",
    "\n",
    "        for i in range(3):\n",
    "            self.downsample_layers.append(nn.Sequential(\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=3, stride=2, padding=1),\n",
    "                LayerNorm(dims[i + 1], eps=1e-6, data_format=\"channels_first\")))\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            main_stage = nn.Sequential(\n",
    "                *[UniRepLKNetBlock(dim=dims[i], kernel_size=kernel_sizes[i][j], drop_path=dp_rates[cur + j],\n",
    "                                   layer_scale_init_value=layer_scale_init_value, deploy=deploy,\n",
    "                                   attempt_use_lk_impl=attempt_use_lk_impl,\n",
    "                                   with_cp=with_cp, use_sync_bn=use_sync_bn) for j in\n",
    "                  range(depths[i])])\n",
    "            self.stages.append(main_stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        last_channels = dims[-1]\n",
    "\n",
    "        self.for_pretrain = init_cfg is None\n",
    "        self.for_downstream = not self.for_pretrain     # there may be some other scenarios\n",
    "        if self.for_downstream:\n",
    "            assert num_classes is None\n",
    "\n",
    "        if self.for_pretrain:\n",
    "            self.init_cfg = None\n",
    "            self.norm = nn.LayerNorm(last_channels, eps=1e-6)  # final norm layer\n",
    "            self.head = nn.Linear(last_channels, num_classes)\n",
    "            self.apply(self._init_weights)\n",
    "            self.head.weight.data.mul_(head_init_scale)\n",
    "            self.head.bias.data.mul_(head_init_scale)\n",
    "            self.output_mode = 'logits'\n",
    "        else:\n",
    "            self.init_cfg = init_cfg        # OpenMMLab style init\n",
    "            self.init_weights()\n",
    "            self.output_mode = 'features'\n",
    "            norm_layer = partial(LayerNorm, eps=1e-6, data_format=\"channels_first\")\n",
    "            for i_layer in range(4):\n",
    "                layer = norm_layer(dims[i_layer])\n",
    "                layer_name = f'norm{i_layer}'\n",
    "                self.add_module(layer_name, layer)\n",
    "\n",
    "\n",
    "    #   load pretrained backbone weights in the OpenMMLab style\n",
    "    def init_weights(self):\n",
    "\n",
    "        def load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "            unexpected_keys = []\n",
    "            own_state = module.state_dict()\n",
    "            for name, param in state_dict.items():\n",
    "                if name not in own_state:\n",
    "                    unexpected_keys.append(name)\n",
    "                    continue\n",
    "                if isinstance(param, torch.nn.Parameter):\n",
    "                    # backwards compatibility for serialized parameters\n",
    "                    param = param.data\n",
    "                try:\n",
    "                    own_state[name].copy_(param)\n",
    "                except Exception:\n",
    "                    raise RuntimeError(\n",
    "                        'While copying the parameter named {}, '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}.'.format(\n",
    "                            name, own_state[name].size(), param.size()))\n",
    "            missing_keys = set(own_state.keys()) - set(state_dict.keys())\n",
    "\n",
    "            err_msg = []\n",
    "            if unexpected_keys:\n",
    "                err_msg.append('unexpected key in source state_dict: {}\\n'.format(', '.join(unexpected_keys)))\n",
    "            if missing_keys:\n",
    "                err_msg.append('missing keys in source state_dict: {}\\n'.format(', '.join(missing_keys)))\n",
    "            err_msg = '\\n'.join(err_msg)\n",
    "            if err_msg:\n",
    "                if strict:\n",
    "                    raise RuntimeError(err_msg)\n",
    "                elif logger is not None:\n",
    "                    logger.warn(err_msg)\n",
    "                else:\n",
    "                    print(err_msg)\n",
    "\n",
    "        logger = get_root_logger()\n",
    "        assert self.init_cfg is not None\n",
    "        ckpt_path = self.init_cfg['checkpoint']\n",
    "        if ckpt_path is None:\n",
    "            print('================ Note: init_cfg is provided but I got no init ckpt path, so skip initialization')\n",
    "        else:\n",
    "            ckpt = _load_checkpoint(ckpt_path, logger=logger, map_location='cpu')\n",
    "            if 'state_dict' in ckpt:\n",
    "                _state_dict = ckpt['state_dict']\n",
    "            elif 'model' in ckpt:\n",
    "                _state_dict = ckpt['model']\n",
    "            else:\n",
    "                _state_dict = ckpt\n",
    "\n",
    "            load_state_dict(self, _state_dict, strict=False, logger=logger)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.output_mode == 'logits':\n",
    "            for stage_idx in range(4):\n",
    "                x = self.downsample_layers[stage_idx](x)\n",
    "                x = self.stages[stage_idx](x)\n",
    "            x = self.norm(x.mean([-2, -1]))\n",
    "            x = self.head(x)\n",
    "            return x\n",
    "        elif self.output_mode == 'features':\n",
    "            outs = []\n",
    "            for stage_idx in range(4):\n",
    "                x = self.downsample_layers[stage_idx](x)\n",
    "                x = self.stages[stage_idx](x)\n",
    "                outs.append(self.__getattr__(f'norm{stage_idx}')(x))\n",
    "            return outs\n",
    "        else:\n",
    "            raise ValueError('Defined new output mode?')\n",
    "\n",
    "    def reparameterize_unireplknet(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'reparameterize'):\n",
    "                m.reparameterize()\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm implementation used in ConvNeXt\n",
    "    LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\", reshape_last_to_first=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "        self.reshape_last_to_first = reshape_last_to_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "#   For easy use as backbone in MMDetection framework. Ignore these lines if you do not use MMDetection\n",
    "if has_mmdet:\n",
    "    @det_BACKBONES.register_module()\n",
    "    class UniRepLKNetBackbone(UniRepLKNet):\n",
    "        def __init__(self,\n",
    "                     depths=(3, 3, 27, 3),\n",
    "                     dims=(96, 192, 384, 768),\n",
    "                     drop_path_rate=0.,\n",
    "                     layer_scale_init_value=1e-6,\n",
    "                     kernel_sizes=None,\n",
    "                     deploy=False,\n",
    "                     with_cp=False,\n",
    "                     init_cfg=None,\n",
    "                     attempt_use_lk_impl=False):\n",
    "            assert init_cfg is not None\n",
    "            super().__init__(in_chans=3, num_classes=None, depths=depths, dims=dims,\n",
    "                             drop_path_rate=drop_path_rate, layer_scale_init_value=layer_scale_init_value,\n",
    "                             kernel_sizes=kernel_sizes, deploy=deploy, with_cp=with_cp,\n",
    "                             init_cfg=init_cfg, attempt_use_lk_impl=attempt_use_lk_impl, use_sync_bn=True)\n",
    "\n",
    "#   For easy use as backbone in MMSegmentation framework. Ignore these lines if you do not use MMSegmentation\n",
    "if has_mmseg:\n",
    "    @seg_BACKBONES.register_module()\n",
    "    class UniRepLKNetBackbone(UniRepLKNet):\n",
    "        def __init__(self,\n",
    "                     depths=(3, 3, 27, 3),\n",
    "                     dims=(96, 192, 384, 768),\n",
    "                     drop_path_rate=0.,\n",
    "                     layer_scale_init_value=1e-6,\n",
    "                     kernel_sizes=None,\n",
    "                     deploy=False,\n",
    "                     with_cp=False,\n",
    "                     init_cfg=None,\n",
    "                     attempt_use_lk_impl=False):\n",
    "            assert init_cfg is not None\n",
    "            super().__init__(in_chans=3, num_classes=None, depths=depths, dims=dims,\n",
    "                             drop_path_rate=drop_path_rate, layer_scale_init_value=layer_scale_init_value,\n",
    "                             kernel_sizes=kernel_sizes, deploy=deploy, with_cp=with_cp,\n",
    "                             init_cfg=init_cfg, attempt_use_lk_impl=attempt_use_lk_impl, use_sync_bn=True)\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    #TODO: it seems that google drive does not support direct downloading with url? so where to upload the checkpoints other than huggingface? any suggestions?\n",
    "}\n",
    "\n",
    "huggingface_file_names = {\n",
    "    \"unireplknet_a_1k\": \"unireplknet_a_in1k_224_acc77.03.pth\",\n",
    "    \"unireplknet_f_1k\": \"unireplknet_f_in1k_224_acc78.58.pth\",\n",
    "    \"unireplknet_p_1k\": \"unireplknet_p_in1k_224_acc80.23.pth\",\n",
    "    \"unireplknet_n_1k\": \"unireplknet_n_in1k_224_acc81.64.pth\",\n",
    "    \"unireplknet_t_1k\": \"unireplknet_t_in1k_224_acc83.21.pth\",\n",
    "    \"unireplknet_s_1k\": \"unireplknet_s_in1k_224_acc83.91.pth\",\n",
    "    \"unireplknet_s_22k\": \"unireplknet_s_in22k_pretrain.pth\",\n",
    "    \"unireplknet_s_22k_to_1k\": \"unireplknet_s_in22k_to_in1k_384_acc86.44.pth\",\n",
    "    \"unireplknet_b_22k\": \"unireplknet_b_in22k_pretrain.pth\",\n",
    "    \"unireplknet_b_22k_to_1k\": \"unireplknet_b_in22k_to_in1k_384_acc87.40.pth\",\n",
    "    \"unireplknet_l_22k\": \"unireplknet_l_in22k_pretrain.pth\",\n",
    "    \"unireplknet_l_22k_to_1k\": \"unireplknet_l_in22k_to_in1k_384_acc87.88.pth\",\n",
    "    \"unireplknet_xl_22k\": \"unireplknet_xl_in22k_pretrain.pth\",\n",
    "    \"unireplknet_xl_22k_to_1k\": \"unireplknet_xl_in22k_to_in1k_384_acc87.96.pth\"\n",
    "}\n",
    "\n",
    "def load_with_key(model, key):\n",
    "    # if huggingface hub is found, download from our huggingface repo\n",
    "    if hf_hub_download is not None:\n",
    "        repo_id = 'DingXiaoH/UniRepLKNet'\n",
    "        cache_file = hf_hub_download(repo_id=repo_id, filename=huggingface_file_names[key])\n",
    "        checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=model_urls[key], map_location=\"cpu\", check_hash=True)\n",
    "    if 'model' in checkpoint:\n",
    "        checkpoint = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "def initialize_with_pretrained(model, model_name, in_1k_pretrained, in_22k_pretrained, in_22k_to_1k):\n",
    "    if in_1k_pretrained:\n",
    "        key = model_name + '_1k'\n",
    "    elif in_22k_pretrained:\n",
    "        key = model_name + '_22k'\n",
    "    elif in_22k_to_1k:\n",
    "        key = model_name + '_22k_to_1k'\n",
    "    else:\n",
    "        key = None\n",
    "    if key:\n",
    "        load_with_key(model, key)\n",
    "\n",
    "@register_model\n",
    "def unireplknet_a(in_1k_pretrained=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_A_F_P_depths, dims=(40, 80, 160, 320), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_a', in_1k_pretrained, False, False)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_f(in_1k_pretrained=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_A_F_P_depths, dims=(48, 96, 192, 384), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_f', in_1k_pretrained, False, False)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_p(in_1k_pretrained=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_A_F_P_depths, dims=(64, 128, 256, 512), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_p', in_1k_pretrained, False, False)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_n(in_1k_pretrained=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_N_depths, dims=(80, 160, 320, 640), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_n', in_1k_pretrained, False, False)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_t(in_1k_pretrained=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_T_depths, dims=(80, 160, 320, 640), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_t', in_1k_pretrained, False, False)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_s(in_1k_pretrained=False, in_22k_pretrained=False, in_22k_to_1k=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_S_B_L_XL_depths, dims=(96, 192, 384, 768), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_s', in_1k_pretrained, in_22k_pretrained, in_22k_to_1k)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_b(in_22k_pretrained=False, in_22k_to_1k=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_S_B_L_XL_depths, dims=(128, 256, 512, 1024), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_b', False, in_22k_pretrained, in_22k_to_1k)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_l(in_22k_pretrained=False, in_22k_to_1k=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_S_B_L_XL_depths, dims=(192, 384, 768, 1536), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_l', False, in_22k_pretrained, in_22k_to_1k)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def unireplknet_xl(in_22k_pretrained=False, in_22k_to_1k=False, **kwargs):\n",
    "    model = UniRepLKNet(depths=UniRepLKNet_S_B_L_XL_depths, dims=(256, 512, 1024, 2048), **kwargs)\n",
    "    initialize_with_pretrained(model, 'unireplknet_xl', False, in_22k_pretrained, in_22k_to_1k)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #   Test case showing the equivalency of Structural Re-parameterization\n",
    "    x = torch.randn(2, 4, 19, 19)\n",
    "    layer = UniRepLKNetBlock(4, kernel_size=13, attempt_use_lk_impl=False)\n",
    "    for n, p in layer.named_parameters():\n",
    "        if 'beta' in n:\n",
    "            torch.nn.init.ones_(p)\n",
    "        else:\n",
    "            torch.nn.init.normal_(p)\n",
    "    for n, p in layer.named_buffers():\n",
    "        if 'running_var' in n:\n",
    "            print('random init var')\n",
    "            torch.nn.init.uniform_(p)\n",
    "            p.data += 2\n",
    "        elif 'running_mean' in n:\n",
    "            print('random init mean')\n",
    "            torch.nn.init.uniform_(p)\n",
    "    layer.gamma.data += 0.5\n",
    "    layer.eval()\n",
    "    origin_y = layer(x)\n",
    "    layer.reparameterize()\n",
    "    eq_y = layer(x)\n",
    "    print(layer)\n",
    "    print(eq_y - origin_y)\n",
    "    print((eq_y - origin_y).abs().sum() / origin_y.abs().sum())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import wget\n",
    "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "# from unireplknet import UniRepLKNet\n",
    "\n",
    "# override the timm package to relax the input shape constraint.\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=224, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "\n",
    "        if verbose == True:\n",
    "            print('---------------AST Model Summary---------------')\n",
    "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "        self.v =  UniRepLKNet(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], drop_path_rate=0.4,\n",
    "                              in_chans=1,\n",
    "                       disable_iGEMM=True)\n",
    "        self.original_embedding_dim = 768\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "    def get_shape(self, fstride, tstride, input_fdim=224, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.v(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     input_tdim = 100\n",
    "#     ast_mdl = ASTModel(input_tdim=input_tdim)\n",
    "#     # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
    "#     test_input = torch.rand([10, input_tdim, 128])\n",
    "#     test_output = ast_mdl(test_input)\n",
    "#     # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
    "#     print(test_output.shape)\n",
    "\n",
    "#     input_tdim = 256\n",
    "#     ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n",
    "#     # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
    "#     test_input = torch.rand([10, input_tdim, 128])\n",
    "#     test_output = ast_mdl(test_input)\n",
    "#     # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
    "#     print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679cdf44-c94c-4f28-85ef-60136ca87ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "=========== use default kernel size \n",
      "((3, 3, 3), (13, 13, 13), (13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3, 13, 3, 3), (13, 13, 13))\n",
      "=========== drop path rates:  [0.0, 0.011428571428571429, 0.022857142857142857, 0.03428571428571429, 0.045714285714285714, 0.05714285714285714, 0.06857142857142857, 0.08, 0.09142857142857143, 0.10285714285714286, 0.11428571428571428, 0.12571428571428572, 0.13714285714285715, 0.14857142857142858, 0.16, 0.17142857142857143, 0.18285714285714286, 0.19428571428571428, 0.2057142857142857, 0.21714285714285714, 0.2285714285714286, 0.24000000000000002, 0.25142857142857145, 0.2628571428571429, 0.2742857142857143, 0.28571428571428575, 0.29714285714285715, 0.3085714285714286, 0.32, 0.33142857142857146, 0.34285714285714286, 0.3542857142857143, 0.3657142857142857, 0.37714285714285717, 0.38857142857142857, 0.4]\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n",
      "---------------- trying to import iGEMM implementation for large-kernel conv\n",
      "---------------- found no iGEMM. use original conv. follow https://github.com/AILab-CVC/UniRepLKNet to install it.\n"
     ]
    }
   ],
   "source": [
    "input_height = features.size()[1]\n",
    "input_width = features.size()[2]\n",
    "model = ASTModel(label_dim=1, fstride=10, tstride=10, input_fdim=input_height, input_tdim=input_width, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True).to(device)\n",
    "# model = Baseline_Model((input_height, input_width)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15e54c8-8449-4890-95ee-a5749982a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "def Bias(output, target):\n",
    "    loss = torch.mean(torch.abs(10**output - 10**target))\n",
    "    return loss\n",
    "\n",
    "def CovStep(output, target, output_mean, target_mean):\n",
    "    loss = torch.mean(((output - output_mean) * (target - target_mean)))\n",
    "    return loss\n",
    "\n",
    "def MeanAbsLogStep(output, target, log=True):\n",
    "    #convert out of log\n",
    "    if log:\n",
    "        vol_pred = 10**output\n",
    "        vol_target = 10**target\n",
    "    else:\n",
    "        vol_pred = output\n",
    "        vol_target = target\n",
    "    loss = torch.mean(torch.abs(torch.log(vol_pred/vol_target)))\n",
    "    return loss\n",
    "\n",
    "def compute_eval_metrics(dataloader, model, log=True):\n",
    "    target_sum = 0\n",
    "    pred_sum = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    for (x,y) in dataloader:        \n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        target_sum += y.sum()\n",
    "        pred_sum += pred.sum()\n",
    "        n_steps += 1\n",
    "    \n",
    "    target_mean = target_sum/n_steps\n",
    "    pred_mean = pred_sum/n_steps\n",
    "    \n",
    "    mse = 0\n",
    "    mean_error = 0\n",
    "    cov = 0\n",
    "    abs_log_ratio = 0\n",
    "    \n",
    "    var_pred = 0 #technically var * N but gets cancelled out in Pearson calculation\n",
    "    var_target = 0 \n",
    "    \n",
    "    for (x,y) in dataloader:        \n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        mse += MSE(pred, y)\n",
    "        mean_error += Bias(pred, y)\n",
    "        cov += CovStep(pred, y, pred_mean, target_mean)\n",
    "        abs_log_ratio += MeanAbsLogStep(pred, y, log=log)\n",
    "        \n",
    "        var_pred += MSE(pred, pred_mean)\n",
    "        var_target += MSE(y, target_mean)\n",
    "        \n",
    "        pears = CovStep(pred, y, pred_mean, target_mean)/(torch.sqrt(MSE(pred, pred_mean))*torch.sqrt(MSE(y, target_mean)))\n",
    "    \n",
    "    out_dict = {}\n",
    "    out_dict['mse'] = (mse / n_steps).item()\n",
    "    out_dict['bias'] = (mean_error / n_steps).item()\n",
    "    out_dict['pearson_cor'] = (cov/(torch.sqrt(var_pred) * torch.sqrt(var_target))).item()\n",
    "    out_dict['mean_mult'] = (torch.exp(abs_log_ratio/n_steps)).item()\n",
    "    \n",
    "    return out_dict\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     eval_dict = compute_eval_metrics(val_dataloader, model)\n",
    "#     print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca9da7-3de4-4f84-a11e-2e7f123691ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tDuration: 2103.09s\tTrain loss: 0.6371\tVal loss: 0.6571\tVal bias:2638.3502\tVal Pearson correlation: 4.6044e-01\tVal MeanMult: 4.3455\n",
      "Epoch: 1\tDuration: 2125.05s\tTrain loss: 0.5750\tVal loss: 0.6197\tVal bias:2618.5975\tVal Pearson correlation: 4.9670e-01\tVal MeanMult: 4.1576\n",
      "Epoch: 2\tDuration: 2148.23s\tTrain loss: 0.5545\tVal loss: 0.6008\tVal bias:2603.1476\tVal Pearson correlation: 5.1809e-01\tVal MeanMult: 4.0621\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(model.parameters(),lr=0.000005, weight_decay=1e-2)\n",
    "\n",
    "hist = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"val_loss\": [],\n",
    "    \"val_bias\": [],\n",
    "    \"val_pearson_cor\": [],\n",
    "    \"val_mean_mult\": []\n",
    "}\n",
    "\n",
    "for ep in range(150):     #########################################################################                   \n",
    "    t_start = time()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_steps = 0\n",
    "    val_steps = 0\n",
    "    \n",
    "    for (x, y) in train_dataloader:\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        loss = MSE(pred, y.reshape((y.shape[0], 1)))\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_steps += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        val_metrics = compute_eval_metrics(val_dataloader, model)\n",
    "    \n",
    "    \n",
    "    hist['train_loss'].append(train_loss/train_steps)\n",
    "    hist['val_loss'].append(val_metrics['mse'])\n",
    "    hist['val_bias'].append(val_metrics['bias'])\n",
    "    hist['val_pearson_cor'].append(val_metrics['pearson_cor'])\n",
    "    hist['val_mean_mult'].append(val_metrics['mean_mult'])\n",
    "    \n",
    "    t_end = time()\n",
    "    \n",
    "    t_elapsed = t_end - t_start\n",
    "    print(\"Epoch: {}\\tDuration: {:.2f}s\\tTrain loss: {:.4f}\\tVal loss: {:.4f}\\tVal bias:{:.4f}\\tVal Pearson correlation: {:.4e}\\tVal MeanMult: {:.4f}\"\\\n",
    "          .format(ep, t_elapsed, train_loss/train_steps, val_metrics['mse'],\\\n",
    "                  val_metrics['bias'], val_metrics['pearson_cor'],val_metrics['mean_mult']))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 创建一个空列表来存储pred和y的值\n",
    "data_to_save = []\n",
    "\n",
    "test1_df = test_df.sample(5)\n",
    "\n",
    "mae = 0.0 \n",
    "total_samples = 0\n",
    "\n",
    "test_dataloader1 = create_dataloader(test1_df) \n",
    "test_random = compute_eval_metrics(test_dataloader1,model) \n",
    "print(test_random)\n",
    "\n",
    "for(x,y) in test_dataloader: \n",
    "    (x,y) = (x.to(device),y.to(device)) \n",
    "    pred = model(x) \n",
    "    for i in range(len(pred)):\n",
    "        data_to_save.append([pred[i].item(), y[i].item()])\n",
    "\n",
    "# 指定要保存的CSV文件名\n",
    "csv_filename = 'predictions.csv'\n",
    "\n",
    "# 打开CSV文件并将数据写入\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # 写入列名（如果需要）\n",
    "    csv_writer.writerow(['Prediction', 'Actual'])\n",
    "    \n",
    "    # 写入数据\n",
    "    csv_writer.writerows(data_to_save)\n",
    "\n",
    "print(f'Data saved to {csv_filename}')\n",
    "#     print(pred,'///',y)\n",
    "\n",
    "#     # 计算绝对误差\n",
    "#     absolute_error = torch.abs(pred-y)\n",
    "\n",
    "#     # 累加绝对误差和样本数\n",
    "#     mae += absolute_error.sum().item()\n",
    "    \n",
    "\n",
    "# #     计算平均绝对误差\n",
    "\n",
    "\n",
    "# mae /= 5 \n",
    "# print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb35a4-183c-43d8-ab9a-00adaba3bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_test = compute_eval_metrics(test_dataloader, model)\n",
    "    print(eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7365bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd7026-863e-496b-bf18-0c22f39c92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.keys()\n",
    "np.std(hist['val_loss'][15:])\n",
    "np.arange(100)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f67288-f36f-431b-8482-e6d4a76d29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = feat_df.sample(20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    random_dataloader = create_dataloader(random_df)\n",
    "    eval_random = compute_eval_metrics(random_dataloader, model)\n",
    "    print(eval_random)\n",
    "\n",
    "    for (x, y) in random_dataloader:\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            pred = model(x)\n",
    "            print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663c8c5-5077-4c39-99dd-e3ab55dba281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class print_Model(Module):\n",
    "    def __init__(self, seq):\n",
    "        super(print_Model, self).__init__()\n",
    "        self.net = seq\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Start\\n{}\".format(x.size()))\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "            print(layer)\n",
    "            print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acb0b5-cf2a-4487-9527-7669b605568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b10cc-e81a-4238-b988-3600a5b6554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = feat_df.sample(20)\n",
    "\n",
    "random_dataloader = create_dataloader(random_df, log=False)\n",
    "\n",
    "\n",
    "load_model = Baseline_Model((input_height, input_width)).to(device)\n",
    "model_name = 'testrun2_regularization'\n",
    "load_model.load_state_dict(torch.load(os.path.join(MODELS_DIR,model_name,'model_state.pt'), map_location=torch.device('cpu')))\n",
    "\n",
    "load_metrics = compute_eval_metrics(test_dataloader, load_model, log=False)\n",
    "for key in load_metrics.keys():\n",
    "    print(key, load_metrics[key])\n",
    "\n",
    "for (x, y) in random_dataloader:\n",
    "    (x, y) = (x.to(device), y.to(device))\n",
    "    pred = load_model(x)\n",
    "    print(pred.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469273db-2880-4c15-a8c5-834d1a0e9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df['vol'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555aab10",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxwnotebook",
   "language": "python",
   "name": "cxwnotebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
